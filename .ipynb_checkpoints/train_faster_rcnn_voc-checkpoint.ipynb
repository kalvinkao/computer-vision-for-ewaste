{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "06. Train Faster-RCNN end-to-end on PASCAL VOC\n================================================\n\nThis tutorial goes through the basic steps of training a Faster-RCNN [Ren15]_ object detection model\nprovided by GluonCV.\n\nSpecifically, we show how to build a state-of-the-art Faster-RCNN model by stacking GluonCV components.\n\nIt is highly recommended to read the original papers [Girshick14]_, [Girshick15]_, [Ren15]_\nto learn more about the ideas behind Faster R-CNN.\nAppendix from [He16]_ and experiment detail from [Lin17]_ may also be useful reference.\n\n.. hint::\n\n    You can skip the rest of this tutorial and start training your Faster-RCNN model\n    right away by downloading this script:\n\n    :download:`Download train_faster_rcnn.py<../../../scripts/detection/faster_rcnn/train_faster_rcnn.py>`\n\n    Example usage:\n\n    Train a default resnet50_v1b model with Pascal VOC on GPU 0:\n\n    .. code-block:: bash\n\n        python train_faster_rcnn.py --gpus 0\n\n    Train a resnet50_v1b model on GPU 0,1,2,3:\n\n    .. code-block:: bash\n\n        python train_faster_rcnn.py --gpus 0,1,2,3 --network resnet50_v1b\n\n    Check the supported arguments:\n\n    .. code-block:: bash\n\n        python train_faster_rcnn.py --help\n\n\n.. hint::\n\n    Since lots of contents in this tutorial is very similar to :doc:`./train_ssd_voc`, you can skip any part\n    if you feel comfortable.\n\n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset\n-------\n\nPlease first go through this `sphx_glr_build_examples_datasets_pascal_voc.py` tutorial to setup Pascal\nVOC dataset on your disk.\nThen, we are ready to load training and validation images.\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gluoncv.data import VOCDetection\n# typically we use 2007+2012 trainval splits for training data\ntrain_dataset = VOCDetection(splits=[(2007, 'trainval'), (2012, 'trainval')])\n# and use 2007 test as validation data\nval_dataset = VOCDetection(splits=[(2007, 'test')])\n\nprint('Training images:', len(train_dataset))\nprint('Validation images:', len(val_dataset))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data transform\n--------------\nWe can read an image-label pair from the training dataset:\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_image, train_label = train_dataset[6]\nbboxes = train_label[:, :4]\ncids = train_label[:, 4:5]\nprint('image:', train_image.shape)\nprint('bboxes:', bboxes.shape, 'class ids:', cids.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the image, together with the bounding box labels:\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\nfrom gluoncv.utils import viz\n\nax = viz.plot_bbox(train_image.asnumpy(), bboxes, labels=cids, class_names=train_dataset.classes)\nplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Validation images are quite similar to training because they were\nbasically split randomly to different sets\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "val_image, val_label = val_dataset[6]\nbboxes = val_label[:, :4]\ncids = val_label[:, 4:5]\nax = viz.plot_bbox(val_image.asnumpy(), bboxes, labels=cids, class_names=train_dataset.classes)\nplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Faster-RCNN networks, the only data augmentation is horizontal flip.\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gluoncv.data.transforms import presets\nfrom gluoncv import utils\nfrom mxnet import nd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "short, max_size = 600, 1000  # resize image to short side 600 px, but keep maximum length within 1000\ntrain_transform = presets.rcnn.FasterRCNNDefaultTrainTransform(short, max_size)\nval_transform = presets.rcnn.FasterRCNNDefaultValTransform(short, max_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "utils.random.seed(233)  # fix seed in this tutorial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We apply transforms to train image\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_image2, train_label2 = train_transform(train_image, train_label)\nprint('tensor shape:', train_image2.shape)\nprint('box and id shape:', train_label2.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Images in tensor are distorted because they no longer sit in (0, 255) range.\nLet's convert them back so we can see them clearly.\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_image2 = train_image2.transpose((1, 2, 0)) * nd.array((0.229, 0.224, 0.225)) + nd.array((0.485, 0.456, 0.406))\ntrain_image2 = (train_image2 * 255).asnumpy().astype('uint8')\nax = viz.plot_bbox(train_image2, train_label2[:, :4],\n                   labels=train_label2[:, 4:5],\n                   class_names=train_dataset.classes)\nplt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Loader\n-----------\nWe will iterate through the entire dataset many times during training.\nKeep in mind that raw images have to be transformed to tensors\n(mxnet uses BCHW format) before they are fed into neural networks.\n\nA handy DataLoader would be very convenient for us to apply different transforms and aggregate data into mini-batches.\n\nBecause Faster-RCNN handles raw images with various aspect ratios and various shapes, we provide a\n:py:class:`gluoncv.data.batchify.Append`, which neither stack or pad images, but instead return lists.\nIn such way, image tensors and labels returned have their own shapes, unaware of the rest in the same batch.\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gluoncv.data.batchify import Tuple, Append\nfrom mxnet.gluon.data import DataLoader\n\nbatch_size = 2  # for tutorial, we use smaller batch-size\nnum_workers = 0  # you can make it larger(if your CPU has more cores) to accelerate data loading\n\n# behavior of batchify_fn: stack images, and pad labels\nbatchify_fn = Tuple(Append(), Append())\ntrain_loader = DataLoader(train_dataset.transform(train_transform), batch_size, shuffle=True,\n                          batchify_fn=batchify_fn, last_batch='rollover', num_workers=num_workers)\nval_loader = DataLoader(val_dataset.transform(val_transform), batch_size, shuffle=False,\n                        batchify_fn=batchify_fn, last_batch='keep', num_workers=num_workers)\n\nfor ib, batch in enumerate(train_loader):\n    if ib > 3:\n        break\n    print('data 0:', batch[0][0].shape, 'label 0:', batch[1][0].shape)\n    print('data 1:', batch[0][1].shape, 'label 1:', batch[1][1].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faster-RCNN Network\n-------------------\nGluonCV's Faster-RCNN implementation is a composite Gluon HybridBlock :py:class:`gluoncv.model_zoo.FasterRCNN`.\nIn terms of structure, Faster-RCNN networks are composed of base feature extraction\nnetwork, Region Proposal Network(including its own anchor system, proposal generator),\nregion-aware pooling layers, class predictors and bounding box offset predictors.\n\n`Gluon Model Zoo <../../model_zoo/index.html>`__ has a few built-in Faster-RCNN networks, more on the way.\nYou can load your favorite one with one simple line of code:\n\n.. hint::\n\n   To avoid downloading model in this tutorial, we set ``pretrained_base=False``,\n   in practice we usually want to load pre-trained imagenet models by setting\n   ``pretrained_base=True``.\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from gluoncv import model_zoo\nnet = model_zoo.get_model('faster_rcnn_resnet50_v1b_voc', pretrained_base=False)\nprint(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faster-RCNN network is callable with image tensor\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import mxnet as mx\nx = mx.nd.zeros(shape=(1, 3, 600, 800))\nnet.initialize()\ncids, scores, bboxes = net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faster-RCNN returns three values, where ``cids`` are the class labels,\n``scores`` are confidence scores of each prediction,\nand ``bboxes`` are absolute coordinates of corresponding bounding boxes.\n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Faster-RCNN network behave differently during training mode:\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from mxnet import autograd\nwith autograd.train_mode():\n    # this time we need ground-truth to generate high quality roi proposals during training\n    gt_box = mx.nd.zeros(shape=(1, 1, 4))\n    cls_preds, box_preds, roi, samples, matches, rpn_score, rpn_box, anchors = net(x, gt_box)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In training mode, Faster-RCNN returns a lot of intermediate values, which we require to train in an end-to-end flavor,\nwhere ``cls_preds`` are the class predictions prior to softmax,\n``box_preds`` are bounding box offsets with one-to-one correspondence to proposals\n``roi`` is the proposal candidates, ``samples`` and ``matches`` are the sampling/matching results of RPN anchors.\n``rpn_score`` and ``rpn_box`` are the raw outputs from RPN's convolutional layers.\nand ``anchors`` are absolute coordinates of corresponding anchors boxes.\n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training losses\n---------------\nThere are four losses involved in end-to-end Faster-RCNN training.\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# the loss to penalize incorrect foreground/background prediction\nrpn_cls_loss = mx.gluon.loss.SigmoidBinaryCrossEntropyLoss(from_sigmoid=False)\n# the loss to penalize inaccurate anchor boxes\nrpn_box_loss = mx.gluon.loss.HuberLoss(rho=1/9.)  # == smoothl1\n# the loss to penalize incorrect classification prediction.\nrcnn_cls_loss = mx.gluon.loss.SoftmaxCrossEntropyLoss()\n# and finally the loss to penalize inaccurate proposals\nrcnn_box_loss = mx.gluon.loss.HuberLoss()  # == smoothl1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RPN training targets\n--------------------\nTo speed up training, we let CPU to pre-compute RPN training targets.\nThis is especially nice when your CPU is powerful and you can use ``-j num_workers``\nto utilize multi-core CPU.\n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we provide network to the training transform function, it will compute training targets\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_transform = presets.rcnn.FasterRCNNDefaultTrainTransform(short, max_size, net)\n# Return images, labels, rpn_cls_targets, rpn_box_targets, rpn_box_masks loosely\nbatchify_fn = Tuple(*[Append() for _ in range(5)])\n# For the next part, we only use batch size 1\nbatch_size = 1\ntrain_loader = DataLoader(train_dataset.transform(train_transform), batch_size, shuffle=True,\n                          batchify_fn=batchify_fn, last_batch='rollover', num_workers=num_workers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time we can see the data loader is actually returning the training targets for us.\nThen it is very naturally a gluon training loop with Trainer and let it update the weights.\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for ib, batch in enumerate(train_loader):\n    if ib > 0:\n        break\n    with autograd.train_mode():\n        for data, label, rpn_cls_targets, rpn_box_targets, rpn_box_masks in zip(*batch):\n            gt_label = label[:, :, 4:5]\n            gt_box = label[:, :, :4]\n            print('data:', data.shape)\n            # box and class labels\n            print('box:', gt_box.shape)\n            print('label:', gt_label.shape)\n            # -1 marks ignored label\n            print('rpn cls label:', rpn_cls_targets.shape)\n            # mask out ignored box label\n            print('rpn box label:', rpn_box_targets.shape)\n            print('rpn box mask:', rpn_box_masks.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RCNN training targets\n---------------------\nRCNN targets are generated with the intermediate outputs with the stored target generator.\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for ib, batch in enumerate(train_loader):\n    if ib > 0:\n        break\n    with autograd.train_mode():\n        for data, label, rpn_cls_targets, rpn_box_targets, rpn_box_masks in zip(*batch):\n            gt_label = label[:, :, 4:5]\n            gt_box = label[:, :, :4]\n            # network forward\n            cls_preds, box_preds, roi, samples, matches, rpn_score, rpn_box, anchors = net(batch[0][0], gt_box)\n            # generate targets for rcnn\n            cls_targets, box_targets, box_masks = net.target_generator(roi, samples, matches, gt_label, gt_box)\n\n            print('data:', data.shape)\n            # box and class labels\n            print('box:', gt_box.shape)\n            print('label:', gt_label.shape)\n            # rcnn does not have ignored label\n            print('rcnn cls label:', cls_targets.shape)\n            # mask out ignored box label\n            print('rcnn box label:', box_targets.shape)\n            print('rcnn box mask:', box_masks.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training loop\n-------------\nAfter we have defined loss function and generated training targets, we can write the training loop.\n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "for ib, batch in enumerate(train_loader):\n    if ib > 0:\n        break\n    with autograd.record():\n        for data, label, rpn_cls_targets, rpn_box_targets, rpn_box_masks in zip(*batch):\n            gt_label = label[:, :, 4:5]\n            gt_box = label[:, :, :4]\n            # network forward\n            cls_preds, box_preds, roi, samples, matches, rpn_score, rpn_box, anchors = net(data, gt_box)\n            # generate targets for rcnn\n            cls_targets, box_targets, box_masks = net.target_generator(roi, samples, matches, gt_label, gt_box)\n\n            # losses of rpn\n            rpn_score = rpn_score.squeeze(axis=-1)\n            num_rpn_pos = (rpn_cls_targets >= 0).sum()\n            rpn_loss1 = rpn_cls_loss(rpn_score, rpn_cls_targets, rpn_cls_targets >= 0) * rpn_cls_targets.size / num_rpn_pos\n            rpn_loss2 = rpn_box_loss(rpn_box, rpn_box_targets, rpn_box_masks) * rpn_box.size / num_rpn_pos\n\n            # losses of rcnn\n            num_rcnn_pos = (cls_targets >= 0).sum()\n            rcnn_loss1 = rcnn_cls_loss(cls_preds, cls_targets, cls_targets >= 0) * cls_targets.size / cls_targets.shape[0] / num_rcnn_pos\n            rcnn_loss2 = rcnn_box_loss(box_preds, box_targets, box_masks) * box_preds.size / box_preds.shape[0] / num_rcnn_pos\n\n        # some standard gluon training steps:\n        # autograd.backward([rpn_loss1, rpn_loss2, rcnn_loss1, rcnn_loss2])\n        # trainer.step(batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ".. hint::\n\n  Please checkout the full :download:`training script <../../../scripts/detection/faster_rcnn/train_faster_rcnn.py>` for complete implementation.\n\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "References\n----------\n\n.. [Girshick14] Ross Girshick and Jeff Donahue and Trevor Darrell and Jitendra Malik. Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation. CVPR 2014.\n.. [Girshick15] Ross Girshick. Fast {R-CNN}. ICCV 2015.\n.. [Ren15] Shaoqing Ren and Kaiming He and Ross Girshick and Jian Sun. Faster {R-CNN}: Towards Real-Time Object Detection with Region Proposal Networks. NIPS 2015.\n.. [He16] Kaiming He and Xiangyu Zhang and Shaoqing Ren and Jian Sun. Deep Residual Learning for Image Recognition. CVPR 2016.\n.. [Lin17] Tsung-Yi Lin and Piotr Dollár and Ross Girshick and Kaiming He and Bharath Hariharan and Serge Belongie. Feature Pyramid Networks for Object Detection. CVPR 2017.\n\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
